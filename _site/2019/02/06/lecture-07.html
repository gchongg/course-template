<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>M340L (change this in .config.yml) | Lecture 7: Maximum likelihood learning of undirected GM</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - Carnegie Mellon University - Spring 2019
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/2019/02/06/lecture-07.html">

  
</head>

    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 7: Maximum likelihood learning of undirected GM",
      "description": "Algorithms for learning UGMs along with a brief overview of CRFs.",
      "published": "February 6, 2019",
      "lecturers": [
        
        {
          "lecturer": "Eric Xing",
          "lecturerURL": "https://www.cs.cmu.edu/~epxing/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Ramesh Balaji"
        },
        
        {
          "author": "Tze Hui Koh"
        },
        
        {
          "author": "Shalom Yiblet",
          "authorURL": "http://yiblet.me"
        },
        
        {
          "author": "Danish Danish",
          "authorURL": "http://cs.cmu.edu/~ddanish"
        }
        
      ],
      "editors": [
        
        {
          "editor": "Maruan Al-Shedivat",
          "editorURL": "https://www.cs.cmu.edu/~mshediva/"
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="http://localhost:4000/">M340L (change this in .config.yml)</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/Syllabus/">Syllabus</a>
        <a class="page-link" href="/lectures/">Lectures</a>
        <a class="page-link" href="/calendar/">Calendar</a>
        <a class="page-link" href="/homework/">Homework</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 7: Maximum likelihood learning of undirected GM</h1>
        <p>Algorithms for learning UGMs along with a brief overview of CRFs.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h3 id="introduction-and-ipf">Introduction and IPF</h3>

<h4 id="mle-for-undirected-graphical-models">MLE for undirected graphical models</h4>

<p>We have an undirected graphical model and we know that using the <strong>Hammersley–Clifford theorem</strong> that it can be represented by a Gibbs distribution.
Now the question arises if we can follow the sane procedure as for directed graphical Models to find the MLE of an UGM?  The answer is no - we cannot.
This is because for directed graphical models the log likelihood decomposes into a sum of terms, one per family i.e. (node + it’s parents).
However the same is not applicable to undirected graphical models, as there is a normalization constant $Z$ that is a function oaf all the parameters and thus the probability distribution does not split into a sum of terms.</p>

<d-math block="">
\begin{aligned}
P \left( x _ { 1 } , \ldots , x _ { n } \right) &amp; = \frac { 1 } { Z } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right) \\
Z &amp; = \sum _ { x _ { 1 } , \ldots , x _ { n } } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right)
\end{aligned}
</d-math>

<p>Although this is a big drawback, we still use undirected graphical models as they are useful - they are used to represent some special cases which cannot be represented by directed graphical models alone. The next section outlines the procedure for finding the MLE through taking the derivative of the log-likelihood and the difficulties that arise by doing so.</p>

<h4 id="log-likelihood-for-ugms">Log Likelihood for UGMs</h4>

<p>Here, we introduce two new quantities <em>Total Count</em> and <em>Clique Count</em>.
<em>Total Count</em> for an Undirected Graphical Model $(V, E)$, is simply the number of times that a configuration x (i.e., $X_v = x$) is observed in a dataset $D = \left{ \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { N } \right}$</p>

<d-math block="">
m ( \mathbf { x } ) = \sum _ { n } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right)
</d-math>

<p>The <em>Clique Count</em> is expressed as a marginal of the <em>Total Count</em>.</p>

<d-math block="">
m \left( \mathbf { x } _ { c } \right) = \sum _ { \mathbf { x } _ { r c } } m ( \mathbf { x } )
</d-math>

<p>Using the counts to express the log likelihood we have,</p>

<d-math block="">
\begin{aligned}
p ( D | \theta ) &amp; = \prod _ { n } \sum _ { x } p ( \mathbf { x } | \theta ) ^ { \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) } \\ \log p ( D | \theta ) &amp; = \sum _ { n } \sum _ { \mathbf { x } } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) \log p ( \mathbf { x } | \theta ) = \sum _ { \mathbf { x } } \sum _ { n } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) \log p ( \mathbf { x } | \theta ) \\ \ell &amp; = \sum _ { \mathbf { x } } m ( \mathbf { x } ) \log \left( \frac { 1 } { Z } \prod _ { c } \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) \\ &amp; = \sum _ { c } \sum _ { \mathbf { x } _ { c } } m \left( \mathbf { x } _ { c } \right) \log \psi _ { c } \left( \mathbf { x } _ { c } \right) - N \log Z
\end{aligned}
</d-math>

<p>Taking the derivative we get,
The first is fairly simple</p>

<d-math block="">
\frac { \partial l _ { 1 } } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = m \left( \mathbf { x } _ { c } \right) / \psi _ { c } \left( \mathbf { x } _ { c } \right)
</d-math>

<p>The second term however is a little more involved,</p>

<d-math block="">
\begin{aligned}
\frac { \partial \log Z } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) }
&amp; = \frac { 1 } { Z } \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \sum _ { \tilde { \mathbf { x } } } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
&amp; = \frac { 1 } { Z } \sum _ { \hat { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
&amp; = \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { 1 } { \psi _ { c } \left( \widetilde { \mathbf { x } } _ { c } \right) } \frac { 1 } { Z } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \\
&amp; = \frac { 1 } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) p ( \widetilde { \mathbf { x } } ) = \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
\end{aligned}
</d-math>

<p>Putting the first and second terms together, the derivative becomes</p>

<d-math block="">
 \frac { \partial \ell } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = \frac { m \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } - N \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
</d-math>

<p>Equating to zero and solving we get,</p>

<d-math block="">
p _ { M L E } ^ { * } \left( \mathbf { x } _ { c } \right) = \frac { m \left( \mathbf { x } _ { c } \right) ^ {  } } { N } = \tilde { p } \left( \mathbf { x } _ { c } \right)
</d-math>

<p>Notice that the $\psi _ { c } \left( \mathbf { x } _ { c } \right)$ term gets cancelled and we are  left with a condition.
In other words, at the maximum likelihood setting of the parameters, for each clique, the model marginals must be equal to the observed marginals (empirical counts).  This doesn’t tell us how to get the ML parameters, it just gives us a condition that must be satisfied when we have them.</p>

<p>Thus we turn to two important algorithms, which we call the <strong>workhorse algorithms</strong>.</p>

<h4 id="iterative-proportional-fitting">Iterative Proportional Fitting</h4>

<p>We have from the derivative of the likelihood this relationship that,</p>

<d-math block="">
\frac { \tilde { p } \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } = \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
</d-math>

<p>Now since $\psi _ { c } \left( \mathbf { x } _ { c } \right)$ turns up on both sides of the equation, solving it in the closed form is impossible. We can however treat this as a fixed point iteration by treating the one on the left hand side as being ahead (uknown) and the one on the right as known, we can then  estimate the one on the left with the one on the right using the equation. We cycle through all the cliques and iterate,</p>

<d-math block="">
\psi _ { c } ^ { ( t + 1 ) } \left( \mathbf { x } _ { c } \right) = \psi _ { c } ^ { ( t ) } \left( \mathbf { x } _ { c } \right) \frac { \tilde { p } \left( \mathbf { x } _ { c } \right) } { p ^ { ( t ) } \left( \mathbf { x } _ { c } \right) }
</d-math>

<p>IPF, is also a co-ordinate ascent algorithm, where the co-ordinates are the parameters of the clique potentials. During each step of the algorithm the log-likelihood increases and thus it will converge to a global maximum.</p>

<h4 id="information-theoretic-viewpoint-on-ipf">Information-theoretic viewpoint on IPF</h4>

<p>Maximizing the log likelihood is equivalent to minimizing the KL divergence from the observed distribution to the model distribution.</p>

<d-math block="">
\max \ell \Leftrightarrow \min K L ( \tilde { p } ( x ) \| p ( x | \theta ) ) = \sum _ { x } \tilde { p } ( x ) \log \frac { \tilde { p } ( x ) } { p ( x | \theta ) }
</d-math>

<h3 id="generalized-iterative-scaling-gis-introduction">Generalized Iterative Scaling (GIS): Introduction</h3>

<p>Now this second algorithm is where we start thinking more about feature-based models. So far, when we mention potential functions, we feel that it is trivial. Either you learn it, or you simply specify numbers at will. But there are some problems. To illustrate this, let’s take a look at a real task of spell checking. One of the most useful and powerful devices in spell checking is to build affinity models of character strings.</p>

<h4 id="feature-based-clique-potentials">Feature-based Clique Potentials</h4>

<p>For example, if we were to look at the consecutive appearance of 3 characters, $c_1, c_2, c_3$, which we represent with the potential function $\psi(x_c)$. The rationale behind this is that we can use this to score the likelihood of misspelling. E.g. If we see the sequence ‘zyz’, we would know that such a sequence does not exist in the English vocabulary and therefore has a very high likelihood of being a misspelling. On the other hand, if we see the sequence ‘ink’, we know that there are some words which contain this sequence and consequently a lower likelihood of misspelling. This is an example of how the potential can help us do spell checking. However, if we were to iterate over all the different combinations of triplets in the alphabet, this would imply that we have $26^3$ features, and this gets worse if we increase the size of the sequence. Such a general “tabular” potential function would be exponentially costly for inference and have exponential numbers of parameters that we must learn from limited data, which would exhaust the computer’s memory.</p>

<p>One solution is to change the graphical model to make cliques smaller. But this changes the dependencies, and may force us to make more independence assumptions than we would like. Another solution is to keep the same graphical model, but use a less general parameterization of the clique potentials. And this is the idea behind feature-based models.</p>

<p><em><strong>Features</strong></em>:
In our earlier example of spell checking, since we know how frequent certain triplets are based on the existing knowledge (e.g. a dictionary or our own vocabulary), we can pick out those we have the greatest confidence in and assign them features. Here a feature is defined as a function which is vacuous for most joint settings except a few particular ones, in which it returns either high or low values. E.g. we can define a feature $f_{ing} = 10$ and $f_{ate} = 9$, and so on and so forth. By the time we get to the 50th or 100th feature, we assume that these combinations are less likely and assign them some arbitrary small likelihood of $\alpha$. We can also define features when the inputs are continuous. Then the idea of having a cell on which the feature is active is no longer relevant, but we might still have a compact parameterization of the feature.</p>

<p>This is a different approach from deep learning where we become overcomplete and hope that the big data will funnel in to get these numbers into place, with no guarantee, because we don’t know how much data is necessary to get the numbers into place, i.e. we don’t know the sample complexity. Instead, here we have a way in which we utilize human knowledge to build a model. And even today, the most successful speech recognition models have their foundations based on this concept.</p>

<p><em><strong>Features as Micropotentials</strong></em>:
Let’s move on to the details. By exponentiating them, each feature function can be made into a “micropotential.” We can multiply these micropotentials together to get a clique potential. For example, a clique potential $\psi(c_1,c_2,c_2)$ could be expressed as:</p>

<d-math block="">
\psi_c (c_1,c_2,c_3) = e^{\theta_{ing} f_{ing}} \times e^{\theta_{?ed} f_{?ed}} \times \dots = \exp \left \{ \sum^K_{k=1} \theta_k f_k(c_1,c_2,c_3) \right \}
</d-math>

<p>There’s still a potential over $26^3$ possible settings, but only uses $K$ parameters if there are $K$ features, and by having one indicator function per combination of $x_c$, we recover the standard tabular potential.</p>

<p><em><strong>Combining Features</strong></em>:
Note that these features can be handcrafted. We also append weights $\theta_k$ in the function because we do not have contextual knowledge of how important each feature is. The marginal over the clique is a generalized exponential family distribution, actually, a GLIM:</p>

<d-math block="">
\psi_c (c_1,c_2,c_3) \propto \exp \left \{ \theta_{ing}f_{ing}(c_1,c_2,c_3) + \theta_{qu?}f_{qu?}(c_1,c_2,c_3) +\theta_{zzz}f_{zzz}(c_1,c_2,c_3) + \dots \right \}
</d-math>

<p>In general, the features may be overlapping, unconstrained indicators or any function of any subset of the clique variables. For example if you have multiple cliques such as overlapping windows over all possible combinations of triplets in our spelling checking example, this does not changed the anatomy of the potential, where we still have a weighted sum of the features.</p>

<d-math block="">
\psi_c(x_c) := \exp \left \{ \sum_{i\in I_c} \theta_k f_k(x_{c_i}) \right \}
</d-math>

<p><em><strong>Feature Based Model</strong></em>:
We can multiply these clique potentials as usual:</p>

<d-math block="">
p(x) = \frac{1}{Z(\theta)} \prod_c \psi_c (x_c) = \frac{1}{Z(\theta)} \exp \left \{\sum_c \sum_{i \in I_c} \theta_k f_k(x_{c_i}) \right \}
</d-math>

<p>However, in general, we can forget about associating features with cliques and just use a simplified form:</p>

<d-math block="">
p(x) = \frac{1}{Z(\theta)}\exp \left \{\sum_{i} \theta_i f_i(x_{c_i}) \right \}
</d-math>

<p>With this redesign, we have the exponential family model, with the features as sufficient statistics. And so we get compactness and prior knowledge. However, the difficulty of learning is greater. Recall that in IPF, we have:</p>

<d-math block="">
\psi_c^{(t+1)} (x_c) = \psi_c^{(t)} \frac{\tilde{p}(x_c)}{p^{(t)}(x_c)}
</d-math>

<p>All we know is that the entire thing can be reweighted, iteratively, but here we have a combination of $\theta$ and $f$, and it is not obvious how we may use this rule to update the weights and features individually.</p>

<p><em><strong>MLE of Feature Based UGMs</strong></em>:
Here let us look at our loss function (scaled likelihood):</p>

<d-math block="">
\begin{aligned}
\tilde{l}(\theta ; D)
&amp;= \frac{l(\theta ;D)}{N} \\
&amp;= \frac{1}{N} \sum_{n} \log p(x_n \mid \theta) \\
&amp;= \sum_x \tilde{p} (x)\log p(x \mid \theta) \\
&amp;= \sum_x \tilde{p} (x)\sum_i \theta_{i} f_i(x) - \log Z(\theta)
\end{aligned}
</d-math>

<p>We introduce $N$ to allow the introduction of an empirical distribution. This is a technicality we can put aside, because $N$ is a constant and does not change the relative numbers in our estimate. The loss function breaks down into two terms where one is a naive sum of $\theta f(x)$, and the other is the more complex $\log Z(\theta)$ with $\theta$ buried deeply into a compound. This is something that we cannot easily take the derivative with respect to.</p>

<p>So instead of attacking this objective directly, we attack its lower bound. The rationale is that often in machine learning we linearize what is non-linear to reduce the complexity. Here we make use of the property that</p>

<d-math block="">
\log Z(\theta) \leq \mu Z(\theta) - \log\mu -1
</d-math>

<p>This bound holds for all $\mu$, in particular, it also holds for $\mu = Z^{-1}(\theta^{(t)})$. As a result, we have</p>

<d-math block="">
\tilde{l}(\theta ; D) \geq \sum_x \tilde{p} (x) \sum_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)}} - \log Z(\theta^{(t)}) + 1
</d-math>

<p>Where we assume that there exists a previous version $\theta^{(t)}$.</p>

<h3 id="generalized-iterative-scaling-algorithm">Generalized Iterative Scaling: Algorithm</h3>
<p>To make our task more explicit, let us establish an explicit relationship; the desired version of $\theta$ versus the current version of $\theta$, called $\Delta\theta$, such that $\Delta \theta_i^{(t)} := \theta_i - \theta_i^{(t)}$. We substitute this into the lower bound of the scaled log-likelihood.</p>

<p>Unfortunately, this is still a difficult expression to deal with. Here $\theta$ is compounded by the summation over all $\theta$, and then exponentiated. It becomes non-linear, and every $\theta$ is coupled with every other $\theta$ because of this operation. But this function has a specific form. It is the exponential of a sum or weighted sum. Let’s switch perspective and treat $f$ as the weights and $\Delta \theta$ as the arguments.</p>

<p>With the assumptions</p>

<d-math block="">
f_i(x) \geq 0, \sum_i f_i(x) = 1
</d-math>

<p>The convexity of the exponential is such that</p>

<d-math block="">
\exp \left( \sum_i \pi_i x_i \right) \leq \sum_i \pi_i \exp \left( x_i \right)
</d-math>

<p>And now we have the sum of the exponentials of each individual $\theta$. This is a commonly used algebraic trick in machine learning.</p>

<d-math block="">
\tilde{l}(\theta;D) \geq \sum_i \theta_i \sum_x \tilde{p}(x)f_i(x) - \sum_x p \left( x \mid \theta^{(t)} \right) \sum_i f_i(x) \exp \left( \Delta \theta_i^{(t)} \right) - \log Z \left( \theta^{(t)} \right) + 1 := \Lambda (\theta)
</d-math>

<p>This is known as GIS. So instead of using the original scaled log-likelihood, we use the lower bound of that which we call $\Lambda (\theta)$. Subsequently we can then take the following standard steps.</p>

<p>Take derivative:</p>

<d-math block="">
\frac{\partial \Lambda}{\partial \theta_i} = \sum_x \tilde{p}(x)f_i(x) - \exp \left( \Delta \theta_i^{(t)} \right) \sum_x p \left( x \mid \theta^{(t)} \right) f_i(x)
</d-math>

<p>Set the derivative to zero</p>

<d-math block="">
e^{\Delta \theta_i^{(t)}} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p \left( x\mid \theta^{(t)} \right)f_i(x)} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)} Z \left( \theta^{(t)} \right)
</d-math>

<p>Where $p^{(t)}(x)$ is the unnormalized version of $p(x \mid \theta^{(t)})$</p>

<d-math block="">
\theta_i^{(t+1)} = \theta_i^{(t)} + \Delta \theta_i^{(t)} \Rightarrow p^{(t+1)}(x) = p^{(t)}(x)\prod_i e^{\Delta \theta_i^{(t)}f_i(x)}
</d-math>

<p>We now have the update equations</p>

<d-math block="">
\begin{aligned}
p^{(t+1)}(x) &amp;= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i \left( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)}) \right)^{f_i(x)} \\
  &amp;= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i \left( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)} \right)^{f_i(x)} \left( Z(\theta^{(t)}) \right)^{\sum_i f_i(x)} \\
  &amp;= p^{(t)}(x)\prod_i \left( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)} \right)^{f_i(x)}
\end{aligned}
</d-math>

<p>Why do we call this scaling? To see this, take a look at the form of the last equation. It is in the form of a ratio calculated over two methods. One is the feature weighted by its empirical probability, and the other is a feature weighted by the inferred probability. In other words, it’s the ratio between what you observe and what you estimate based on what you have now, and we are essentially rescaling the expression iteratively like this.</p>

<h3 id="summary-of-ipf-and-gis">Summary of IPF and GIS</h3>
<p>In sum, IPF is a general algorithm for finding the MLE of UGMs. It has a fixed point equation for $\psi_c$ single cliques which do not have to be max-cliques, I-projection in the clique marginal space, requires the potential to be fully parameterized, and in the case of a fully decomposable model reduces to a single step iteration.</p>

<p>On the other hand, GIS is an iterative scaling on general UGM with feature-based potentials. IPF is, in fact, a special case of GIS in which the clique potential is built on features defined as an indicator function of clique configurations.</p>

<h3 id="the-exponential-family">The Exponential Family</h3>

<p>An exponential distribution is a distribution where the probability mass function (or density function for continious distributions) takes the form</p>

<d-math block="">
\begin{aligned}
f_{X}(x) = h(x)g(\theta) exp(\eta(\theta) \cdot T(x))
\end{aligned}
</d-math>

<p>where $\eta(\theta) \vdot T(x))$ is the dot product of two vector-valued functions $\eta, T$ of the same dimensionality $n$.</p>

<p>These class of distributions come up often in machine learning, and other applications. Some of the most common distributions are exponential. The normal distribution, the exponential distribution, the chi-squared, the bernoulli, the multinomial, the poisson, and even the geometric distribution are all members of the exponential family.</p>

<h4 id="the-pitman-koopman-darmois-theorem">The Pitman-Koopman-Darmois Theorem</h4>
<p>One interesting thing about the exponential family is that is has a unique property among all families of distributions. For all families of distributions with fixed support, only exponential families have
a sufficient statistic whose dimension remains bounded as the sample
size increases. In essence if one can find a “reasonable” sufficient
statistic from observing a sample distribution with fixed support
the distribution <strong>must</strong> be in the exponential family.</p>

<h4 id="why-does-the-exponential-family-come-up-so-frequently">Why does the exponential family come up so frequently?</h4>

<p>One possible explanation comes from optimization.
Say we are trying to derive an underlying distribution from observing a sample distribution $p$ and a dataset $X$.
Say that we have some fixed expected values for each some set of $i$ features.</p>

\[\sum_{x} p(x)f_i(x) = \alpha_i\]

<p>Assuming these expectations are consistent (ie we don’t say that the mean is  both 1 and 2), there may exist many distributions that satisfy this constraint. We should select <em>the most uncertain one</em>. The one where the entropy $E[-log(p(x))]$ is maximized.</p>

<p>With this constraints, we can derive a simple optimization problem:</p>

<d-math block="">
\begin{aligned}
    max_p - &amp;\sum_{x}p(x)log(p(x)) \\
    s.t. &amp;\sum_{x}p(x)f_i(x) = \alpha_i \\
    &amp;\sum_{x}p(x) = 1 \\
\end{aligned}
</d-math>

<p>Using these constraints we can derive the best solution to this optimization problem:</p>

<figure>
  <div class="row">
    <div class="col two">
      <img src="/assets/img/notes/lecture-07/slide27.png" />
    </div>
  </div>
  <figcaption>
    <strong>taken from slide 27 of the lectures</strong>
  </figcaption>
</figure>

<p>We have shown that assuming the distribution maximizes the entropy while matching some empirical feature averages forces the optimal solution to be in the exponential family. Interestingly, we can also prove this in the other direction.</p>

<p>Assuming that our solution is the MLE solution in the exponential family, we can derive that it will match empirical feature average</p>

<figure>
  <div class="row">
    <div class="col two">
      <img src="/assets/img/notes/lecture-07/slide25.png" />
    </div>
  </div>
  <figcaption>
    <strong>taken from slide 25 of the lectures</strong>
  </figcaption>
</figure>

<h3 id="conditional-random-fields-crfs">Conditional Random Fields (CRFs)</h3>

<p>In this section, we will first motivate CRFs and discuss how they overcome the shortcomings of HMMs. Second, we will briefly describe its inference and learning mechanism. Next, we will mention some of its applications, and empirical successes. Lastly, we will conclude with some of the recent research on hybrid neural-CRFs.</p>

<figure>
  <div class="row">
    <div class="col three">
      <img src="/assets/img/notes/lecture-07/crfs.png" />
    </div>
  </div>
  <figcaption>
    <strong>Example of a 1-D chain CRF model.</strong>
  </figcaption>
</figure>

<h4 id="introduction">Introduction</h4>

<p>The graphical representation of a 1-D chain CRF is depicted in the figure above. CRFs can be thought of as an evolution of HMMs, but unlike HMMs they are discriminative models, and directly
model the conditional distribution $P(\mathbf{Y}|\mathbf{X})$ instead of the joint distribution $P(\mathbf{X}, \mathbf{Y})$.
Most often, we care about the prediction anyway, so modeling it directly avoids the computation of the joint probability.</p>

<p>CRFs offer two key advantages over HMMs:</p>

<ul>
  <li>They model the entire observed sequence. For example, in a task where we want to predict the Part-Of-Speech (POS) tags for a sequence of words in a sentence, it helps to know the entire sequence of words while predicting the POS tag at time step $t$.</li>
  <li>They do not suffer from the label bias problem. We briefly discuss the problem below.</li>
</ul>

<figure>
  <div class="row">
    <div class="col two">
      <img src="/assets/img/notes/lecture-07/label_bias.png" />
    </div>
  </div>
  <figcaption>
    <strong>An example to demonstrate the label bias problem.</strong>
  </figcaption>
</figure>

<p><strong>The Label Bias Problem</strong>:
In the figure above, we can observe that state 1 transitions into only two states (state 1, and 2); whereas state 2 can potentially transition into all the five available states. Furthermore, we observe that at every time step, state 1 always prefers to transition to state 2 over state 1, and similarly state 2 always prefers to transition to state 2 over other 4 options. However, path <code class="language-plaintext highlighter-rouge">1 -&gt; 1 -&gt; 1 -&gt; 1</code> is the most likely path with probability of $0.09$, whereas the <em>intuitively correct</em> path of <code class="language-plaintext highlighter-rouge">1 -&gt; 2 -&gt; 2 -&gt; 2</code> has a probability mass of only $0.054$. The reason for this counter-intuitive phenomena is rooted in the fact that probability mass from state 2 is distributed across all states. Thus, the probabilities being locally normalized is not desirable here. CRFs overcome this problem by having <em>potential scores</em> instead of probabilities, which are <em>globally normalized</em>.</p>

<p>Having motivated why such models might be useful, let us see to learn and do inference over these models.</p>

<h4 id="inference-and-learning">Inference and learning</h4>

<p>The inference algorithm for CRFs is very simple, and just like HMMs, we can do Viterbi decoding.</p>

<p>The task of learning is essentially learning the weights ($\mathbf{w}$) for the features $\mathbf{f}(y_i, y_{i-1}, \mathbf{x}_{1:n})$ used to score the configurations. One can draw parallels here to a well known class of discriminative classifiers — logistic regression, and just like logistic regression, we can do gradient ascent over the weights to increase the conditional probability over the observed set of $\mathbf{X}, \mathbf{Y}$ pairs. Given the 1-D chain structure, the normalizing term (or the partition function) can be tractably computed. In practice, gradient ascent has very slow convergence, and other alternatives like conjugate gradient method and limited memory quasi-newton methods can be used for faster learning.
Most CRFs, especially ones involving arbitrary structure, are intractable in general owing to the difficulty in computing the partition function.</p>

<p><strong>Applications</strong>: CRFs have been very successful in a wide range of applications across different fields. In NLP, they are commonly used for POS-tagging, Named Entity Recognition (NER), and other tagging problems. In computer vision, they have found utility in image segmentation, image restoration, handwriting recognition among other tasks.</p>

<p><strong>Recent developments</strong>: A popular recent trend in NLP tagging problems is to combine CRFs with neural representations from CNNs, and LSTMs instead of the hand-crafted features $\mathbf{f}(y_i, y_{i-1}, \mathbf{x}_{1:n})$, and the weights for the CNNs and LSTMs are also learnt as a part of the learning process (through back-propagation).</p>

<p>Lastly, a tutorial <d-cite key="sutton2012introduction"></d-cite> from Sutton, and McCallum is an excellent resource to know more in-depth details about CRFs</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Matrices and Matrix Calculation</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jacky Chong</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2022 Carnegie Mellon University. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/assets/bibliography/2019-02-06-lecture-07.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>







<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
