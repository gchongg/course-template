<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>M340L (change this in .config.yml) | Lecture 21: Sequential decision making (part 2): The algorithms</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - Carnegie Mellon University - Spring 2019
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/2019/04/03/lecture-21.html">

  
</head>

    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 21: Sequential decision making (part 2): The algorithms",
      "description": "An introduction for max-entropy RL algorithms",
      "published": "April 3, 2019",
      "lecturers": [
        
        {
          "lecturer": "Maruan Al-Shedivat",
          "lecturerURL": "https://www.cs.cmu.edu/~mshediva/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Xingyu Lin",
          "authorURL": "https://xingyu-lin.github.io/"
        },
        
        {
          "author": "Mengxiong Liu"
        },
        
        {
          "author": "Junyan Jiang"
        }
        
      ],
      "editors": [
        
        {
          "editor": "Paul Liang"
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="http://localhost:4000/">M340L (change this in .config.yml)</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/Syllabus/">Syllabus</a>
        <a class="page-link" href="/lectures/">Lectures</a>
        <a class="page-link" href="/calendar/">Calendar</a>
        <a class="page-link" href="/homework/">Homework</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 21: Sequential decision making (part 2): The algorithms</h1>
        <p>An introduction for max-entropy RL algorithms</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h2 id="recap-control-as-inference">Recap: Control as Inference</h2>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="/assets/img/notes/lecture-21/recap_control.png" />
        </div>
    </div>
</figure>

<p>In Control setting:</p>

<ul>
  <li>Initial State \(s_0 \sim p_0(s)\)</li>
  <li>Transition \(s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)\)</li>
  <li>Policy \(a_{t+1} \sim \pi(a_t \mid s_t)\)</li>
  <li>Reward \(r_t \sim r(s_t, a_t)​\)</li>
</ul>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="/assets/img/notes/lecture-21/recap_inference.png" />
        </div>
    </div>
</figure>

<p>In Inference setting:</p>

<ul>
  <li>Initial State \(s_0 \sim p_0(s)\)</li>
  <li>Transition \(s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)\)</li>
  <li>Policy \(a_{t+1} \sim \pi(a_t \mid s_t)\)</li>
  <li>Reward \(r_t \sim r(s_t, a_t)\)</li>
  <li>Optimality \(p(\mathcal{O_t} = 1 \mid s_t, a_t) = exp(r(s_t, a_t))\)</li>
</ul>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="/assets/img/notes/lecture-21/recap_classical.png" />
        </div>
    </div>
</figure>

<p>In the classical deterministic RL setup, we have:</p>

<d-math block="">
\begin{aligned} 
&amp; V_\pi(s) := \mathbb{E}_\pi \displaystyle \sum_{k=0}^T \left[\gamma^k r_{t+k+1} \mid s_t = s \right]\\ 
&amp; Q_\pi (s, a) := \mathbb{E}_\pi \displaystyle \sum_{k=0}^T \left[\gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]\\ 
&amp; \pi_*(a \mid s) := \delta(a = \underset{a}{\text{argmax}} Q_*(s,a))\\
\end{aligned}
</d-math>

<p>Let \(V_t(s_t) = \log \beta_t(s_t), Q_t(s_t, a_t) = \log \beta_t(s_t, a_t), V(s_t) = \log \int exp(Q(s_t, a_t) + \log p(a_t \mid s_t)) da_t​\). 
Denote \(\tau = (s_1,a_1, …, s_T,a_T)​\) as the full trajectory. Denote $p(\tau) = p(\tau \mid \mathcal{O}_{1:T})​$. Running inference in this GM allows us to compute:</p>

<d-math block="">
\begin{aligned} 
&amp; p(\tau \mid \mathcal{O}_{1:T}) \propto p(s_t) \displaystyle \prod_{t=2}^T p(s_{t+1} \mid s_t, a_t) \times exp(\displaystyle \sum_{t=1}^T r(s_t, a_t)) \\ 
&amp; p(a_t \mid s_t, \mathcal{O}_{1:T}) \propto exp(Q_t(s_t, a_t) - V_t(s_t))​ \\
\end{aligned}
</d-math>

<p>From the perspective of control as inference, we optimize for the following objective:</p>

\[-D_{KL}(\hat{p}(\tau) \mid\mid p(\tau)) = \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim \hat p(s_t, a_t)}\left[r(s_t, a_t)\right] + \mathbb{E}_{s_t \sim \hat{p}(s_t)}\left[\mathcal{H}(\pi(a_t \mid s_t))\right]​\]

<ul>
  <li>For deterministic dynamics, we get this objective directly.</li>
  <li>For stochastic dynamics, we obtain it from the ELBO on the evidence.</li>
</ul>

<h2 id="types-of-rl-algorithms">Types of RL Algorithms</h2>

<p>The objective of RL learning is to find the optimal parameter s.t. maximize the expected reward.</p>

\[\theta^*=\arg\max_\theta \mathbb{E}_{\tau\sim p(\tau)}\left[\sum_{t=1}^T r(s_t,a_t)\right]\]

<ul>
  <li>Policy gradients: directly optimize the above stochastic objective</li>
  <li>Value-based: estimate V-function or Q-function of the optimal policy (no explicit policy; the policy is derived from the value function)</li>
  <li>Actor-critic: estimate V-/Q-function under the current policy and use it toimprove the policy (not covered)</li>
  <li>Model-based methods: not covered</li>
</ul>

<h2 id="policy-gradients">Policy gradients</h2>

<p>In policy gradient, we directly optimize the target expected reward w.r.t the policy $\pi_\theta$ itself.</p>

\[\begin{aligned}
J(\theta)&amp;=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(s_{t}, a_{t}\right)\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} r\left(s_{i, t}, a_{i, t}\right) \\
\nabla_{\theta} J(\theta) &amp;=\nabla_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[r(\tau)]=\int r(\tau) \nabla_{\theta} p_{\theta}(\tau) d \tau=\int r(\tau) p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) d \tau \\ &amp;=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[r(\tau) \nabla_{\theta} \log p_{\theta}(\tau)\right] 
\end{aligned}\]

<p>The log-derivative trick is applied to the above equation.
\(\begin{array}{l}{\nabla_{\theta} \log p_{\theta}(\tau)=\nabla_{\theta}\left[\log p\left(s_{1}\right)+\sum_{t=1}^{T} \log p\left(s_{t+1} | s_{t}, a_{t}\right)+\log \pi_{\theta}\left(a_{t} | s_{t}\right)\right]} \\ {\nabla_{\theta} J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(s_{t}, a_{t}\right)\right)\right]}\end{array}\)</p>

<p>The reinforce algorithm:
\(\begin{array}{l}{\text { 1. sample }\left\{\tau_{i}\right\}_{i=1}^{N} \text { under } \pi_{\theta}\left(a_{t} | s_{t}\right)} \\ {\text { 2. } \hat{J}(\theta)=\sum_{i}\left(\sum_{t} \log \pi_{\theta}\left(a_{i, t} | s_{i, t}\right)\right)\left(\sum_{t} r\left(s_{i, t}, a_{i, t}\right)\right)} \\ {\text { 3. } \theta \leftarrow \theta+\alpha \nabla_{\theta} \hat{J}(\theta)}\end{array}\)</p>
<h2 id="q-learning">Q-Learning</h2>

<p>Q-learning does not explicitly optimize the policy \(\pi_\theta\); it optimize the estimation of the \(V,Q\) functions. The optimal policy can then be calculated by
\(\pi^{\prime}\left(a_{t} | s_{t}\right)=\delta\left(a_{t}=\arg \max _{a}\left[Q_{\pi}\left(a, s_{t}\right)\right]\right)\)</p>
<h3 id="policy-iteration-via-dynamic-programming">Policy iteration via dynamic programming:</h3>

<ul>
  <li>
    <p>Policy iteration
\(\begin{array}{l}{\text { 1. evaluate } Q_{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[V_{\pi}\left(s^{\prime}\right)\right]} \\ {\text { 2. update } \pi \leftarrow \pi^{\prime}}\end{array}\)</p>
  </li>
  <li>
    <p>Policy evaluation
\(V_{\pi}(s) \leftarrow r(s, \pi(s))+\gamma \mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, \pi(s)\right)}\left[V_{\pi}\left(s^{\prime}\right)\right]\)</p>
  </li>
</ul>

<p>The approach still involves explicit optimization of \(\pi_\theta\). We can rewrite the iteration as:</p>

\[\begin{array}{l}{\text { 1. set } Q(s, a) \leftarrow r(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[V\left(s^{\prime}\right)\right]} \\ {\text { 2. } \operatorname{set} V(s) \leftarrow \max _{a} Q(s, a)}\end{array}\]

<h3 id="fitted-q-learning">Fitted Q-learning:</h3>

<p>If the state space is high-dimensional or infinite, it is not feasible to represent \(Q, V\) in a tabular form. In this case, we use two parameterized functions \(Q_\phi, V_\phi\) to denote them. Then, we adopt fitted Q-iteration as stated in <a href="http://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf">this paper</a>:</p>

\[\begin{array}{l}{\text { 1. set } y_{i} \leftarrow r\left(s_{i}, a_{i}\right)+\gamma \mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[V_{\phi}\left(s_{i}^{\prime}\right)\right]} \\ {\text { 2. } \operatorname{set} \phi \leftarrow \arg \min _{\phi} \sum_{i}\left\|Q_{\phi}\left(s_{i}, a_{i}\right)-y_{i}\right\|^{2}}\end{array}\]

<h2 id="soft-policy-gradients">Soft Policy Gradients</h2>
<p>From the perspective of control as inference, we optimize for the following objective:</p>

<d-math block="">
\begin{aligned}
J(\theta) &amp;= -D_{KL}(\hat{p}(\tau) || p(\tau)) \\
&amp;= \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim \hat{p(s_t, a_t)}}[r(s_t, a_t)] + \mathbb{E}_{s_t \sim \hat{p}(s_t)}[\mathcal{H}(\pi(a_t|s_t))]\\
&amp;= \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[r(s_t, a_t) - \log \pi(a_t|s_t)]\\
\end{aligned}​
</d-math>

<p>Now following the policy gradient method, such as REINFORCE, we just need to add a bonus entropy term to the rewards.</p>

<h2 id="soft-q-learning">Soft Q-learning</h2>
<p>Next, we connect the previous policy gradient to Q-learning. We can rewrite the policy gradient as follows:</p>
<d-math block="">
\begin{aligned}
J(\theta) &amp;= -D_{KL}(\hat{p}(\tau) || p(\tau)) \\
&amp;= \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim \hat{p(s_t, a_t)}}[r(s_t, a_t)] + \mathbb{E}_{s_t \sim \hat{p}(s_t)}[\mathcal{H}(\pi(a_t|s_t))]\\
&amp;= \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[r(s_t, a_t) - \log \pi(a_t|s_t)]\\
\end{aligned}
</d-math>

<p>Note that</p>
<d-math block="">
\begin{aligned}
\nabla_\theta \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[\log \pi(a_t|s_t)] &amp;= \int \nabla_\theta \big[ p(\tau) \sum_{t=1}^T \log \pi(a_t|s_t)\big] d\tau\\
&amp;= \int \nabla_\theta p(\tau)\sum_{t=1}^T \log \pi(a_t|s_t) + p(\tau) \nabla_\theta\sum_{t=1}^T \log \pi(a_t|s_t) d\tau\\
&amp;= \int p(\tau) \nabla_\theta \log p(\tau)\sum_{t=1}^T \log \pi(a_t|s_t) + p(\tau) \nabla_\theta \log p(\tau) d\tau\\
&amp;= \int p(\tau) \nabla_\theta \log p(\tau)\Big [ \sum_{t=1}^T \log \pi(a_t|s_t) + 1 \Big] d\tau.\\
\end{aligned}
</d-math>

<p>Recall from the previous lecture,
\(\pi(a_t|s_t) = p(a_t | s_t, \mathcal{O}_{1:T}) = \exp(Q_\theta(s_t, a_t) - V_\theta(s_t))\)</p>

<p>\(V(s_t) = \log \int \exp(Q(s_t, a_t))da_t = \text{softmax}_{a_t} Q(s_t, a_t).\)
Now combine these and rearrange the terms, we get:</p>
<d-math block="">
\begin{aligned}
\nabla_\theta J(\theta) &amp;= \nabla_\theta \sum_{t=1}^T \mathbb{E}_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[r(s_t, a_t) - \log \pi(a_t|s_t)]\\
&amp;\approx \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \nabla_\theta \log\pi_\theta(a_t|s_t) \Big[ r(s_t, a_t) + \big(\sum_{t'=t+1}^T r(s_{t'}, a_{t'}) - \log \pi_\theta(a_{t'}|s_{t'})\big) - \log \pi_\theta (a_t|s_t) - 1 \Big] \\
&amp;= \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \Big(\nabla_\theta Q_\theta(s_t, a_t) - \nabla_\theta V_\theta(s_t)\Big) \Big[ r(s_t, a_t) +  Q_\theta(s_{t'}, a_{t'}) - Q_\theta(s_t, a_t) + V(s_t) \Big] \\
&amp;\approx \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \nabla_\theta Q_\theta(s_t, a_t) \Big[ r(s_t, a_t) + \text{soft} \max_{a_{t'}}  Q_\theta(s_{t'}, a_{t'}) - Q_\theta(s_t, a_t) \Big] \\
\end{aligned}
</d-math>

<p>Now the soft Q-learning update is very similar to Q-learning:
\(\theta \gets \theta + \alpha\nabla_\theta Q_\theta(s,a)(r(s,a) + \gamma V(s') - Q_\theta(s,a)),\)
where
\(V(s') = \text{soft}\max_{a'} Q_\theta(s', a') = \log \int \exp (Q_\theta(s', a')) da'.\)
Additionally, we can set the temperature in softmax to control the tradeoff between entropy and rewards.</p>

<p>To summaize, there are a few benefits of soft optimality:</p>
<ul>
  <li>Improve exploration and prevent entropy collapse</li>
  <li>Easier to specialize (finetune) policies for more specific tasks</li>
  <li>Principled approach to break ties</li>
  <li>Better robustness (due to wider coverage of states)</li>
  <li>Can reduce to hard optimality as reward magnitude increases</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Matrices and Matrix Calculation</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jacky Chong</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2022 Carnegie Mellon University. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/assets/bibliography/2019-04-03-lecture-21.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>







<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
